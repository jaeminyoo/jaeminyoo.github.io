%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Medium Length Professional CV
% LaTeX Template
% Version 2.0 (8/5/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Trey Hunner (http://www.treyhunner.com/)
%
% Important note:
% This template requires the resume.cls file to be in the same directory as the
% .tex file. The resume.cls file provides the resume style used for structuring the
% document.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{resume} % Use the custom resume.cls style

\usepackage[left=0.75in,top=0.6in,right=0.75in,bottom=1in]{geometry} % Document margins
\usepackage{enumitem}
\usepackage{comment}

% Left alignment without word splitting
\usepackage[document]{ragged2e}
\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

\makeatletter
\newcommand \Dotfill {\leavevmode \cleaders \hb@xt@ .99em{\hss .\hss }\hfill \kern \z@}
\makeatother

\name{Jaemin Yoo} % Your name
\address{Postdoctoral Research Fellow @ Carnegie Mellon University} % Your phone number and email
\address{\textbf{Email:} jaeminyoo@cmu.edu \\ \textbf{Web:} jaeminyoo.github.io}

\begin{document}

%----------------------------------------------------------------------------------------
%	POSITIONS
%----------------------------------------------------------------------------------------

\begin{rSection}{Position}

\textbf{Carnegie Mellon University}, Pittsburgh, PA, USA \hfill \emph{Mar. 2022 - Present} \\ 
Postdoctoral Research Fellow, Heinz College of Information Systems and Public Policy \\
Advisor: \emph{Prof. Leman Akoglu}

\end{rSection}

%----------------------------------------------------------------------------------------
%	EDUCATION
%----------------------------------------------------------------------------------------

\begin{rSection}{Education}

\textbf{Seoul National University}, Seoul, South Korea \hfill \emph{Mar. 2016 - Feb. 2022} \\ 
Ph.D. in Computer Science and Engineering \\
Advisor: \emph{Prof. U Kang} \\
Thesis: \emph{Probabilistic Approaches for Node and Graph Classification}
%\emph{Received the Best Ph.D. Thesis Award in SNU CSE}

\textbf{Seoul National University}, Seoul, South Korea \hfill \emph{Mar. 2012 - Feb. 2016} \\ 
B.S. in Computer Science and Engineering

\end{rSection}

%----------------------------------------------------------------------------------------
%	RESEARCH INTERESTS
%----------------------------------------------------------------------------------------

\begin{rSection}{Research Interests}

\begin{enumerate}[noitemsep, leftmargin=*]
	\item \textbf{Self-supervised Anomaly Detection:} Showed that the alignment between augmentation and anomalies is the key to the success of self-supervised learning for anomaly detection [arXiv-22a].
%	\item Studied the role and effect of data augmentation on self-supervised learning for anomaly detection [arXiv-22a]. 
	\medskip
	\item \textbf{Machine Learning on Graphs:} Designed inference-based approaches for node classification on edge-attributed graphs [ICDM-17], cold-start inductive learning [IJCAI-19], PU learning [ICDM-21], and missing feature estimation [KDD-22].
	Proposed a way to modify the structure of a real-world graph for tractable inference [WSDM-20] or graph classification [WWW-22]. 
	Improved the accuracy, interpretability, and robustness of graph neural networks via linearization [arXiv-22b].
	\medskip
	\item \textbf{Interpretable ML:} Improved the learning capacity and interpretability of tree models by fusion with deep learning [ICDM-19, PAKDD-21].
	Proposed a unified representation of deep tree models [SDM-22].
	Understood the function of a deep neural network without data [NeurIPS-19].
	\medskip
	\item \textbf{Multivariate time Series Forecasting:} Learned the relationships between time series variables by attention [SDM-21] or data-axis Transformer specifically for the financial domain [KDD-21].
\end{enumerate}

\end{rSection}

%----------------------------------------------------------------------------------------
%	AWARDS
%----------------------------------------------------------------------------------------

\begin{rSection}{Awards \& Honors}

Best Ph.D. Thesis Award in SNU CSE\smallskip \Dotfill \emph{Feb. 2022} \\ 
One of the Best-Ranked Papers of ICDM 2021\smallskip \Dotfill \emph{Dec. 2021} \\ 
SNU BK21 Outstanding Graduate Student Award\smallskip \Dotfill \emph{Jul. 2021} \\ 
SIAM Student Travel Award (SDM 2021) \smallskip \Dotfill \emph{Apr. 2021} \\ 
SNU BK21 Star Researcher Award \smallskip \Dotfill \emph{Feb. 2021} \\ 
Qualcomm Innovation Fellowship \smallskip \Dotfill \emph{Dec. 2020} \\ 
Yulchon AI Star Award \smallskip \Dotfill \emph{Sep. 2020} \\ 
Google PhD Fellowship (Machine Learning) \smallskip \Dotfill \emph{Sep. 2019} \\ 
Samsung HumanTech Paper Award (Honorable Mention) \smallskip \Dotfill \emph{Feb. 2019} \\ 
Google Conference Scholarship (ICDM 2017) \smallskip \Dotfill \emph{Nov. 2017} \\ 

\end{rSection}
\newpage

%----------------------------------------------------------------------------------------
%	STRENGTHS
%----------------------------------------------------------------------------------------

%\begin{rSection}{Strengths}
%
%\textbf{Honors \& Fellowships} \\
%\begin{itemize}[noitemsep]
%	\item \textbf{Received} Google PhD Fellowship (2019) and Qualcomm Innovation Fellowship (2020) \smallskip
%	\item \textbf{Received} Best Ph.D. Thesis Award (2022) from Dept. of CSE from Seoul National Univ. \smallskip
%	\item (Our work is) \textbf{Selected} as one of the best-ranked papers of ICDM 2021
%\end{itemize}
%
%\textbf{Professional Services} \\ 
%\begin{itemize}[noitemsep]
%	\item \textbf{To Give} tutorials about hypergraphs on DM conferences: ICDM, CIKM, and DSAA (2022) \smallskip
%	\item \textbf{Served as Session Chair} (KDD),
%	\textbf{PC Member} (KDD, AAAI, SDM, BigComp),
%	\textbf{Journal Reviewer} (Pattern Recognition), and
%	\textbf{External Conference Reviewer} (ICLR, NeurIPS, WWW, KDD, BigComp, CIKM, WSDM, ICDM, SAC)
%\end{itemize}
%
%\textbf{Teaching and Development}
%\begin{itemize}[noitemsep]
%	\item \textbf{Worked as a TA} in Large Data Analysis, Data Mining, and Data Structure classes (in SNU) \smallskip
%	\item \textbf{Developed} distributed machine learning algorithms on Apache Spark (w/ SK Telecom) \smallskip
%	\item \textbf{Developed} recommender and anomaly detector systems (w/ SK Broadband, NCSOFT, etc.)
%\end{itemize}
%
%\end{rSection}

%----------------------------------------------------------------------------------------
%	RESEARCH INTERESTS
%----------------------------------------------------------------------------------------

%\begin{rSection}{Research Interests}
%
%\begin{enumerate}[leftmargin=*]
%	\item \textbf{Self-supervised Anomaly Detection} \smallskip \\
%		I study the role and effect of data augmentation on self-supervised learning for anomaly detection.
%		Our preprint [arXiv-22] shows that the alignment between augmented data and unseen anomalies plays an essential role in accuracy and determines the success of self-supervised learning.
%	\item \textbf{Probabilistic Graph Mining} \smallskip \\
%		Probabilistic modeling allows us to effectively parameterize the relationships between nodes even with insufficient observations.
%		I designed inference-based methods for node classification on edge-attributed graphs [ICDM-17], cold-start inductive learning with unseen isolated test nodes [IJCAI-19], positive-unlabeled (PU) learning [ICDM-21], and missing feature estimation [KDD-22].
%	\item \textbf{Structural Modification of Graphs} \smallskip \\
%		The structure of a graph can be modified for the better performance of graph algorithms in terms of accuracy or efficiency.
%		I proposed BTW [WSDM-20] for bounded-treewidth graph sampling, and NodeSam and SubMix [WWW-22] for model-agnostic graph augmentation.
%	\item \textbf{Interpretable Machine Learning} \smallskip \\
%		Tree-structured decisions provide global interpretability but with limited learning capacity.
%		Our study allows one to build a compact [ICDM-19] and interpretable [PAKDD-21] soft decision tree whose structure can be effectively searched via unified representation [SDM-22].
%		On the other hand, I designed a framework to understand the function of a deep neural network even without any training data by distilling its knowledge as a form of artificial data [NeurIPS-19].
%	\item \textbf{Multivariate Time Series Forecasting} \smallskip \\
%		Variables in a multivariate time series make correlated movements over time, and the estimation of such correlations provides a significant gain in forecasting accuracy.
%		I designed a lightweight framework for general forecasting [SDM-21] and specialized it to the financial domain [KDD-21].
%\end{enumerate}
%
%\end{rSection}
%\clearpage

%----------------------------------------------------------------------------------------
%	RESEARCH INTERESTS
%----------------------------------------------------------------------------------------

%\begin{rSection}{Research Statement}
%
%\textbf{Self-supervised Anomaly Detection}
%\begin{itemize}
%	\item I study the role and effect of data augmentation in the success of self-supervised learning (SSL) on anomaly detection, which has gained a lot of attention recently.
%	\begin{itemize}
%		\item Our preprint [arXiv-22] shows that the alignment between augmented samples and unseen anomalies plays an essential role in detection accuracy, and imperfect alignment even harms the accuracy of an unsupervised baseline model.
%		We show that SSL is not \emph{magic}.
%	\end{itemize}
%\end{itemize}
%
%\textbf{Probabilistic Graph Mining}
%\begin{itemize}
%	\item Probabilistic modeling allows us to effectively parameterize the stochastic property of a graph and to develop efficient solutions for challenging problems often with insufficient data.
%	\begin{itemize}
%		\item SBP [ICDM-17] and BPN [IJCAI-19] solve node classification by modeling a real graph as a pairwise Markov network.
%		GRAB [ICDM-21] solves positive-unlabeled (PU) learning in the EM framework, while SVGA [KDD-22] generalizes variational inference to graph data.
%%		\item SBP [ICDM-17] models an edge-attributed graph as a pairwise Markov network and learns its potential matrix by utilizing loopy belief propagation as a differentiable operator.
%%		\item BPN [IJCAI-19] solves node classification tasks in cold-start inductive settings by modeling predictions of classifiers as differentiable priors of a Markov network.
%%		\item GRAB [ICDM-21] trains a graph neural network in the positive-unlabeled (PU) learning setting throudjgh the expectation-maximization (EM) framework.
%%		\item SVGA [KDD-22] generalizes variational inference to graph-structured data by modeling the correlations between of latent variables based on the graph Laplacian.
%	\end{itemize}
%\end{itemize}
%
%\textbf{Structural Modification of Graphs}
%\begin{itemize}
%	\item The structure of a real-world graph can be modified, optimized, or randomly augmented for a better performance of graph algorithms in terms of accuracy or efficiency.
%	\begin{itemize}
%		\item BTW [WSDM-20] samples a subgraph with bounded treewidth to support tractable and accurate inference of marginal probabilities.
%		NodeSam and SubMix [WWW-22] augment a graph structure in a model-agnostic way, improving the accuracy of graph classifiers.
%%		\item BTW [WSDM-20] samples a subgraph with bounded treewidth from a large real graph to support tractable and effective inference of marginal probabilities.
%%		\item NodeSam and SubMix [WWW-22] are model-agnostic augmentation algorithms that help the training of any graph classifier by performing systematic graph augmentation.
%	\end{itemize}
%\end{itemize}
%
%\textbf{Interpretable Deep Tree Models}
%\begin{itemize}
%	\item Tree-structured decision processes provide global interpretability but have inherent limitation in representation power.
%	I study to improve tree models by fusion with deep learning.
%	\begin{itemize}
%		\item EDiT [ICDM-19] provides a way to distill the knowledge of a large ensemble of trees into a compact soft decision tree, and GSDT [PAKDD-21] improves its limited interpretability by using  low-rank Gaussian mixtures as a decision function.
%		TART [SDM-22] generalizes such  deep tree models as a single unified framework to find an optimal tree structure.
%%		\item EDiT [ICDM-19] provides a way to distill the knowledge of a large ensemble of trees into a compact soft decision tree.
%%		\item GSDT [PAKDD-21] improves the limited interpretability of soft decision trees with interpretable low-rank Gaussian mixtures as internal branches.
%%		\item TART [SDM-22] is a unified representation of deep tree models as a chain of matrix-vector multiplications, and supports effective search for an optimal tree structure.
%	\end{itemize}
%\end{itemize}
%
%\textbf{Multivariate Time Series Forecasting}
%\begin{itemize}
%	\item Variables in a multivariate time series make correlated movements over time, and the correct estimation of such correlations results in a significant gain in forecasting accuracy.
%	\begin{itemize}
%		\item AttnAR [SDM-21] is a lightweight framework that allows attention functions of various types to be used to learn the correlations.
%		DTML [KDD-22] focuses on the dynamic and asymmetric relationships between companies in a stock market.
%%		\item AttnAR [SDM-21] is a simple and lightweight framework that allows attention functions of various types to be used to learn the correlations between target variables.
%%		\item DTML [KDD-22] learns the dynamic and asymmetric relationships between companies in a stock market by running data-axis Transformer on top of multi-level contexts.
%	\end{itemize}
%\end{itemize}
%
%\textbf{Zero-shot Knowledge Distillation}
%\begin{itemize}
%	\item The shortage of training data is the core challenge in various applications.
%	Many of my previous works studied data shortage in different perspectives, e.g., PU learning.
%	\begin{itemize}
%		\item KegNet [NeurIPS-19] is a framework to distill the knowledge of a trained neural network without accessing any training data.
%		Artificial data generated in the process of knowledge distillation allow one to understand the internal function of the network.
%	\end{itemize}
%\end{itemize}
%
%\end{rSection}
%\clearpage

%----------------------------------------------------------------------------------------
%	PREPRINTS
%----------------------------------------------------------------------------------------

\begin{rSection}{Preprints}
\smallskip
\begin{enumerate}[leftmargin=*]

	\item[{[i2]}]
		SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success \\
		\underline{Jaemin Yoo}*, Meng-Chieh Lee*, Shubhranshu Shekhar, and Christos Faloutsos \\
		\textbf{arXiv Preprint} (2022; *equal contribution) \\

	\item[{[i1]}]
		Understanding the Effect of Data Augmentation in Self-supervised Anomaly Detection \\
		\underline{Jaemin Yoo}, Tiancheng Zhao, and Leman Akoglu \\
		\textbf{arXiv Preprint} (2022) \\

\end{enumerate}
\end{rSection}

%----------------------------------------------------------------------------------------
%	TUTORIALS
%----------------------------------------------------------------------------------------

\begin{rSection}{TUtorials}
\smallskip
\begin{enumerate}[leftmargin=*]

	\item[{[t1]}]
		Mining of Real-world Hypergraphs: Concepts, Patterns, and Generators \\
		Geon Lee, \underline{Jaemin Yoo}, and Kijung Shin \\
		\textbf{ICDM 2022} / \textbf{CIKM 2022} / \textbf{DSAA 2022} \\

\end{enumerate}
\end{rSection}

%----------------------------------------------------------------------------------------
%	PUBLICATIONS
%----------------------------------------------------------------------------------------

\begin{rSection}{Publications}
\smallskip
\begin{enumerate}[leftmargin=*]

	\item[{[c15]}]
		Reciprocity in Directed Hypergraphs: Measures, Findings, and Generators \\
		Sunwoo Kim, Minyoung Choe, \underline{Jaemin Yoo}, and Kijung Shin \\
		\textbf{ICDM 2022} (acceptance rate 174/890 = 19.6\%) \\

	\item[{[j3]}]
		Graph-based PU Learning for Binary and Multiclass Classification without Class Prior \\
		\underline{Jaemin Yoo}*, Junghun Kim*, Hoyoung Yoon*, Geonsoo Kim, Changwon Jang, and U Kang \\
		\textbf{Knowledge and Information Systems} (SCIE Journal, 2022; *equal contribution) \\

	\item[{[c14]}]
		Accurate Node Feature Estimation with Structured Variational Graph Autoencoder \\
		\underline{Jaemin Yoo}, Hyunsik Jeon, Jinhong Jung, and U Kang \\
		\textbf{KDD 2022} (acceptance rate 254/1695 = 15.0\%) \\

	\item[{[j2]}]
		Signed Random Walk Diffusion for Effective Representation Learning in Signed Graphs \\
		Jinhong Jung, \underline{Jaemin Yoo}, and U Kang \\
		\textbf{PLOS ONE} (SCIE Journal, 2022)

	\item[{[d1]}]
		Probabilistic Approaches for Node and Graph Classification \\
		\underline{Jaemin Yoo} \\
		\textbf{Ph.D. Thesis}, Seoul National University, 2022 \\
		\emph{Received the Best Ph.D. Thesis Award in SNU CSE}

	\item[{[c13]}]
		Model-Agnostic Augmentation for Accurate Graph Classification \\
		\underline{Jaemin Yoo}, Sooyeon Shim, and U Kang \\
		\textbf{WWW 2022} (acceptance rate 323/1822 = 17.7\%) \\

	\item[{[c12]}]
		MiDaS: Representative Sampling from Real-world Hypergraphs \\
		Minyoung Choe, \underline{Jaemin Yoo}, Geon Lee, Woonsung Baek, U Kang, and Kijung Shin \\
		\textbf{WWW 2022} (acceptance rate 323/1822 = 17.7\%) \\

	\item[{[c11]}]
		Transition Matrix Representation of Trees with Transposed Convolutions \\
		\underline{Jaemin Yoo} and Lee Sael \\
		\textbf{SDM 2022} (acceptance rate 83/298 = 27.8\%) \\

	\item[{[c10]}]
		Accurate Graph-Based PU Learning without Class Prior \\
		\underline{Jaemin Yoo}*, Junghun Kim*, Hoyoung Yoon*, Geonsoo Kim, Changwon Jang, and U Kang \\
		\textbf{ICDM 2021} (regular paper; top 98/990 = 9.9\%; *equal contribution) \\
		\emph{Selected as one of the best-ranked papers of ICDM 2021 for fast-track journal invitation}

	\item[{[c9]}]
		Accurate Multivariate Stock Movement Prediction via Data-Axis Transformer with Multi-Level Contexts  \\
		\underline{Jaemin Yoo}, Yejun Soun, Yong-chan Park, and U Kang \\
		\textbf{KDD 2021} (acceptance rate 238/1541 = 15.4\%)

	\item[{[c8]}]
		Gaussian Soft Decision Trees for Interpretable Feature-Based Classification \\
		\underline{Jaemin Yoo} and Lee Sael \\
		\textbf{PAKDD 2021} (acceptance rate 157/768 = 20.4\%)
		
	\item[{[c7]}]
		Attention-Based Autoregression for Accurate and Efficient Multivariate Time Series Forecasting \\
		\underline{Jaemin Yoo} and U Kang \\
		\textbf{SDM 2021} (acceptance rate 85/400 = 21.3\%)

	\item[{[c6]}]
		Sampling Subgraphs with Guaranteed Treewidth for Accurate and Efficient Graphical Inference \\
		\underline{Jaemin Yoo}, U Kang, Mauro Scanagatta, Giorgio Corani, and Marco Zaffalon \\
		\textbf{WSDM 2020} (acceptance rate 91/615 = 14.8\%)

	\item[{[c5]}]
		Knowledge Extraction with No Observable Data \\
		\underline{Jaemin Yoo}, Minyong Cho, Taebum Kim, and U Kang \\
		\textbf{NeurIPS 2019} (acceptance rate 1428/6743 = 21.2\%)

	\item[{[c4]}]
		EDiT: Interpreting Ensemble Models via Compact Soft Decision Trees \\
		\underline{Jaemin Yoo} and Lee Sael \\
		\textbf{ICDM 2019} (acceptance rate 194/1046 = 18.5\%)

	\item[{[c3]}]
		Belief Propagation Network for Hard Inductive Semi-Supervised Learning \\
		\underline{Jaemin Yoo}, Hyunsik Jeon, and U Kang \\
		\textbf{IJCAI 2019} (acceptance rate 850/4752 = 17.9\%)

	\item[{[c2]}]
		Fast and Scalable Distributed Loopy Belief Propagation on Real-World Graphs \\
		Saehan Jo, \underline{Jaemin Yoo}, and U Kang \\
		\textbf{WSDM 2018} (acceptance rate 83/514 = 16.3\%)

	\item[{[j1]}]
		Efficient Learning of Bounded-Treewidth Bayesian Networks from Complete and Incomplete Data Sets \\
		Mauro Scanagatta, Giorgio Corani, Marco Zaffalon, \underline{Jaemin Yoo}, and U Kang \\
		\textbf{International Journal of Approximate Reasoning} (SCIE Journal, 2018)

	\item[{[c1]}]
		Supervised Belief Propagation: Scalable Supervised Inference on Attributed Networks \\
		\underline{Jaemin Yoo}, Saehan Jo, and U Kang \\
		\textbf{ICDM 2017} (regular paper; top 72/778 = 9.3\%)

\end{enumerate}
\end{rSection}

%----------------------------------------------------------------------------------------
%	INVITED TALKS
%----------------------------------------------------------------------------------------

\begin{rSection}{Invited Talks}

SNU AI Summer School 2022, Online \smallskip \Dotfill \emph{Aug. 2022} \\ 
SK C\&C, Online \smallskip \Dotfill \emph{Aug. 2022} \\ 
KAIST School of Computing, Online \smallskip \Dotfill \emph{Jul. 2022} \\ 
AWS Deep Learning Group, Online \smallskip \Dotfill \emph{Jul. 2022} \\ 
EIRIC Seminar, Online \smallskip \Dotfill \emph{Apr. 2022} \\ 
KAIST School of Electrical Engineering, Daejeon, South Korea \smallskip \Dotfill \emph{Feb. 2022} \\ 
LG AI Research Tech Talk, Seoul, South Korea \smallskip \Dotfill \emph{Feb. 2022} \\ 
KAIST AI Student Colloquium, Online \smallskip \Dotfill \emph{Oct. 2021} \\ 
SNU AI Summer School 2021, Online \smallskip \Dotfill \emph{Aug. 2021} \\ 
SNU AI Institute (AIIS) Retreat 2021, Seoul, South Korea \smallskip \Dotfill \emph{Apr. 2021} \\ 
NAVER Online Tech Talk, Online \smallskip \Dotfill \emph{Dec. 2020} \\ 
SNU AI Summer School 2020, Seoul, South Korea \smallskip \Dotfill \emph{Aug. 2020} \\ 
SNU Hospital, Seoul, South Korea \smallskip \Dotfill \emph{Jul. 2020} \\ 
SNU AI Institute (AIIS) Retreat 2020, Seoul, South Korea \smallskip \Dotfill \emph{Jun. 2020}\\ 
Kakao Enterprise, Seongnam, South Korea \smallskip \Dotfill \emph{Jan. 2020} \\ 
Korea Software Congress (KSC) 2019, Pyeongchang, South Korea \smallskip \Dotfill \emph{Dec. 2019} \\ 
SNU Center for AI (SCAI) Retreat 2019, Chuncheon \smallskip \Dotfill \emph{Jul. 2019} \\ 
Samsung Electronics, Suwon, South Korea \smallskip \Dotfill \emph{Mar. 2019} \\ 
IDSIA, Lugano, Switzerland \smallskip \Dotfill \emph{Jul. 2018} \\ 
Korea Software Congress 2017, Busan, South Korea \smallskip \Dotfill \emph{Dec. 2017}

\end{rSection}

%----------------------------------------------------------------------------------------
%	PROJECTS
%----------------------------------------------------------------------------------------

\begin{comment}
\begin{rSection}{Developments}

Graph-Based Gold Farming Group Detection System (w/ NCSOFT) \smallskip \Dotfill \emph{Sep. 2020 - Feb. 2021} \\ 
Deep Learning-Based Recommender System (w/ Wemakeprice) \smallskip \Dotfill \emph{Feb. 2019 - Dec. 2019} \\ 
Statistical Learning and Inference Method with PGMs (w/ IDSIA) \smallskip \Dotfill \emph{Jan. 2016 - Dec. 2018} \\ 
Feature Selection Method for RNN Recommender Systems (w/ SK Telecom) \smallskip \Dotfill \emph{Mar. 2018 - Nov. 2018} \\ 
Temporal Stock Price Prediction System (w/ eMoney) \smallskip \Dotfill \emph{Aug. 2017 - Feb. 2018} \\ 
Video Recommender System with Multimodal Data (w/ SK Broadband) \smallskip \Dotfill \emph{Nov. 2016 - Jun. 2017} \\ 
Distributed Machine Learning Library on Apache Spark (w/ SK Telecom) \smallskip \Dotfill \emph{Mar. 2016 - Jan. 2017}

\end{rSection}
\end{comment}

%----------------------------------------------------------------------------------------
%	PROFESSIONAL SERVICES
%----------------------------------------------------------------------------------------

\begin{rSection}{Miscellaneous}

\textbf{Professional Services}
\begin{itemize}[noitemsep]
	\item Session Chair: KDD 2022 \smallskip
	\item Program Committee: AAAI 2021-2023, BigComp 2021-2023, KDD 2021-2022, SDM 2022 \smallskip
	\item Journal Reviewer: Pattern Recognition  (2021-2022) \smallskip
	\item External Reviewer: ICLR 2021-2022, NeurIPS 2020-2021, WWW 2018-2021, KDD 2018-2020, BigComp 2017-2020, CIKM 2017-2019, WSDM 2019, ICDM 2018, SAC 2018
\end{itemize}

\textbf{Developments}
\begin{itemize}[noitemsep]
	\item Anomaly Detector System in MMORPG (w/ NCSOFT) \smallskip \Dotfill \emph{Sep. 2020 - Feb. 2021} \\ 
	\item Recommender System in E-commerce (w/ Wemakeprice) \smallskip \Dotfill \emph{Feb. 2019 - Dec. 2019} \\ 
	\item Statistical Learning and Inference Method with PGMs (w/ IDSIA) \smallskip \Dotfill \emph{Jan. 2016 - Dec. 2018} \\ 
	\item Feature Selection Method for Recommender Systems (w/ SK Telecom) \smallskip \Dotfill \emph{Mar. 2018 - Nov. 2018} \\ 
	\item Temporal Stock Price Prediction System (w/ eMoney) \smallskip \Dotfill \emph{Aug. 2017 - Feb. 2018} \\ 
	\item Temporal Video Recommender System (w/ SK Broadband) \smallskip \Dotfill \emph{Nov. 2016 - Jun. 2017} \\ 
	\item Distributed ML Library on Apache Spark (w/ SK Telecom) \smallskip \Dotfill \emph{Mar. 2016 - Jan. 2017}
\end{itemize}

\textbf{Teaching Assistant} (Seoul National University)
\begin{itemize}[noitemsep]
	\item Large Data Analysis (M1522.000900, 002) \smallskip \Dotfill \emph{Fall 2017}
	\item Introduction to Data Mining (M1522.001400\_001) \smallskip \Dotfill \emph{Spring 2017}
	\item Data Structures (M1522.001600\_002) \Dotfill \emph{Fall 2016}
\end{itemize}

\textbf{Teaching Assistant} (Other Organizations)
\begin{itemize}[noitemsep]
	\item Deep Learning, Samsung Electronics \smallskip \Dotfill \emph{Apr. 2018 - Feb. 2019}
	\item Deep Learning, SNU Fourth Industrial Revolution Academy \smallskip \Dotfill \emph{Oct. 2017 - Dec. 2018}
	\item Distributed Computing, SNU Big Data Academy \smallskip \Dotfill \emph{Feb. 2017 - Dec. 2017}
	\item Distributed Computing, SNU Big Camp \smallskip \Dotfill \emph{Aug. 2016 - Feb. 2017}
\end{itemize}

\end{rSection}

%----------------------------------------------------------------------------------------
%	EXPERIENCE
%----------------------------------------------------------------------------------------

\begin{comment}
\begin{rSection}{Working Experience}

\textbf{Infosys Limited} (Bengaluru, India) \hfill \emph{Jun. 2015 - Jul. 2015} \\ 
Student Intern \\
Project: \emph{Prediction of CPU and memory usage in cloud services}

\textbf{SK Hynix} (Incheon, South Korea) \hfill \emph{Dec. 2014 - Jan. 2015} \\ 
Student Intern \\
Project: \emph{Documentation and improvement of SSD controller systems}

\textbf{KISTI} (Daejeon, South Korea) \hfill \emph{Jan. 2014 - Feb. 2014} \\ 
Student Intern \\
Project: \emph{Exploiting HTCaaS for traffic simulation}

\end{rSection}
\end{comment}

%----------------------------------------------------------------------------------------
%	TEACHING EXPERIENCE
%----------------------------------------------------------------------------------------

\begin{comment}
\begin{rSection}{Teaching Experience}

\textbf{Teaching Assistant} (Seoul National University)
\begin{itemize}[noitemsep]
	\item Large Data Analysis (M1522.000900, 002) \smallskip \Dotfill \emph{Fall 2017}
	\item Introduction to Data Mining (M1522.001400\_001) \smallskip \Dotfill \emph{Spring 2017}
	\item Data Structures (M1522.001600\_002) \Dotfill \emph{Fall 2016}
\end{itemize}

\textbf{Teaching Assistant} (Other Organizations)
\begin{itemize}[noitemsep]
	\item Deep Learning, Samsung Electronics \smallskip \Dotfill \emph{Apr. 2018 - Feb. 2019}
	\item Deep Learning, SNU Fourth Industrial Revolution Academy \smallskip \Dotfill \emph{Oct. 2017 - Dec. 2018}
	\item Distributed Computing, SNU Big Data Academy \smallskip \Dotfill \emph{Feb. 2017 - Dec. 2017}
	\item Distributed Computing, SNU Big Camp \smallskip \Dotfill \emph{Aug. 2016 - Feb. 2017}
\end{itemize}

\end{rSection}
\end{comment}

%----------------------------------------------------------------------------------------
%	PATENTS
%----------------------------------------------------------------------------------------

%\begin{rSection}{Patents}
%\smallskip
%\begin{enumerate}
%
%	\item \underline{Jaemin Yoo} and U Kang, ``Apparatus and Method for Classifying Nodes", KR-Registration No. 10-1924832 (2018)
%		
%	\item Taebum Kim, \underline{Jaemin Yoo}, and U Kang, ``Method for Compressing Deep Learning Neural Networks and Apparatus for Performing the Same", KR-Registration No. 10-2199285 (2020)
%
%	\item \underline{Jaemin Yoo} and U Kang, ``Method for Extracting Knowledge from Artificial Neural Network and Apparatus for Performing the Same", KR-Registration No. 10-2345262 (2021)
%		
%	\item \underline{Jaemin Yoo}, Sooyeon Shim, and U Kang, ``Apparatus and Method for Data Augmentation", KR-Application No. 10-2021-0169909 (2021)
%		
%	\item \underline{Jaemin Yoo}, Hyunsik Jeon, Jinhong Jung, and U Kang, ``Apparatus and Method for Predicting Feature of Node", KR-Application No. 10-2021-0172385 (2021)
%
%\end{enumerate}
%\end{rSection}

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\end{document}
